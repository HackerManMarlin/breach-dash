name: scrape

on:
  schedule:
    - cron: '*/15 * * * *'          # every 15 min
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with: {python-version: '3.11'}

      - name: Install deps
        run: pip install -r scrapers/requirements.txt

      - name: Scrape & ingest
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
        run: python scrapers/main.py

      - name: Update GitHub Pages
        if: github.ref == 'refs/heads/main'
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git checkout gh-pages
          git checkout main -- dashboard/index.html
          mv dashboard/index.html .
          git rm -rf dashboard || true
          git add index.html
          git commit -m "Update dashboard with latest changes" || echo "No changes to commit"
          git push origin gh-pages
